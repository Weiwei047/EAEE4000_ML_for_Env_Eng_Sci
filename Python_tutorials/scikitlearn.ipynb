{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Examples feat. Pandas and Keras\n",
    "\n",
    "ORIGINAL SOURCE : https://github.com/raspstephan/ESS-Python-Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:03:18.305433Z",
     "start_time": "2018-05-14T19:03:16.471025Z"
    }
   },
   "source": [
    "## [1] Basic Linear Regression with Pandas and Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.262260Z",
     "start_time": "2018-05-14T21:13:06.892616Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some volcano eruption data from http://volcano.si.edu/database/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.278214Z",
     "start_time": "2018-05-14T21:13:07.263988Z"
    }
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 490, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGVP_Eruption_Results.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1253\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1253\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 490, saw 7\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"GVP_Eruption_Results.csv\", header=0, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.295401Z",
     "start_time": "2018-05-14T21:13:07.279806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's a get a broad sense of the data\n",
    "print('n_records:', len(df))\n",
    "print('n_unique_volcanos:', len(df['Volcano Name'].unique()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the number of eruptions by year. It's pretty easy to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.557502Z",
     "start_time": "2018-05-14T21:13:07.296982Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('Start Year').count().plot(legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hmmm... OK well it looks like volcanic eruption data just wasn't good until recently. Let's get just the last 100 years or so, and exclude this year since it's just started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.565019Z",
     "start_time": "2018-05-14T21:13:07.559512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick side-note, if we rename the columns to have no spaces, we can jupyter's convenient autocompletion feature\n",
    "old_cols = df.columns\n",
    "df.rename(columns=lambda x: x.lower().replace(' ', '_'), inplace=True)\n",
    "for i, col in enumerate(df.columns): print(old_cols[i], col, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.581264Z",
     "start_time": "2018-05-14T21:13:07.567159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset the data into a new dataframe\n",
    "recent_df = df[(df.start_year > 1918) & (df.start_year < 2018)]\n",
    "print('n_records:', len(recent_df))\n",
    "print('n_unique_volcanos:', len(recent_df.volcano_name.unique()))\n",
    "recent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.735734Z",
     "start_time": "2018-05-14T21:13:07.582907Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recent_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecent_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_year\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mplot(legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recent_df' is not defined"
     ]
    }
   ],
   "source": [
    "recent_df.groupby('start_year').count().plot(legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is better, but recording data still probably improved in the 1950's. Still, let's just use this anyway for a simple linear regression to predict the next 10 years or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:07.748014Z",
     "start_time": "2018-05-14T21:13:07.737875Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recent_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's just make a quick dataframe that is just the years with their value counts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m regdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mrecent_df\u001b[49m\u001b[38;5;241m.\u001b[39mstart_year\u001b[38;5;241m.\u001b[39mvalue_counts())\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      3\u001b[0m regdf\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_eruptions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m regdf\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recent_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's just make a quick dataframe that is just the years with their value counts\n",
    "regdf = pd.DataFrame(recent_df.start_year.value_counts()).reset_index()\n",
    "regdf.columns = ['year', 'n_eruptions']\n",
    "regdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:08.154188Z",
     "start_time": "2018-05-14T21:13:07.749472Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_model\n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mregdf\u001b[49m\u001b[38;5;241m.\u001b[39myear\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m regdf\u001b[38;5;241m.\u001b[39mn_eruptions\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m reg \u001b[38;5;241m=\u001b[39m linear_model\u001b[38;5;241m.\u001b[39mLinearRegression()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regdf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "x = regdf.year.values.reshape(-1, 1)\n",
    "y = regdf.n_eruptions.values.reshape(-1, 1)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x, y)\n",
    "year_range = np.arange(x.min(), 2031, 1).reshape(-1, 1)\n",
    "\n",
    "plt.scatter(x, y, s=7)\n",
    "plt.plot(year_range, reg.predict(year_range), color='blue', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:03:18.305433Z",
     "start_time": "2018-05-14T19:03:16.471025Z"
    }
   },
   "source": [
    "## [2] Basic K-Nearest Neighbors with Pandas and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use the basic Iris dataset to illustrate KNN. This code is based off of the KNN example at http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:08.182641Z",
     "start_time": "2018-05-14T21:13:08.156579Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:08.191980Z",
     "start_time": "2018-05-14T21:13:08.184624Z"
    }
   },
   "outputs": [],
   "source": [
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# let's only use the first two features, sepal length and sepal width\n",
    "x = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:08.198976Z",
     "start_time": "2018-05-14T21:13:08.194174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=15, weights='uniform')\n",
    "clf.fit(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:08.318815Z",
     "start_time": "2018-05-14T21:13:08.201131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.arange(x[:, 0].min(), x[:, 0].max()+0.2, 0.2),\n",
    "                     np.arange(x[:, 1].min(), x[:, 1].max()+0.2, 0.2))\n",
    "z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, edgecolor='k', s=20, cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:03:18.305433Z",
     "start_time": "2018-05-14T19:03:16.471025Z"
    }
   },
   "source": [
    "## [3] Flow chart of Good Scikit-Learn Functions to Use Based on Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:03:18.305433Z",
     "start_time": "2018-05-14T19:03:16.471025Z"
    }
   },
   "source": [
    "## [4] Performing a K-Fold Cross-Validation on a Keras Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on code at: https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes we might want to cross-validate our neural network model.  Scikit-learn has a super convenient classes to help do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:09.437620Z",
     "start_time": "2018-05-14T21:13:08.320687Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:09.454251Z",
     "start_time": "2018-05-14T21:13:09.439488Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "pimadata.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dataset is based on health data for Pima Indian population with diabetes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpimadata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split into input (X) and output (Y) variables\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m8\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/numpy/lib/npyio.py:1313\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1311\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1313\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/numpy/lib/npyio.py:955\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    953\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 955\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras/lib/python3.10/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: pimadata.csv not found."
     ]
    }
   ],
   "source": [
    "# Dataset is based on health data for Pima Indian population with diabetes\n",
    "dataset = np.loadtxt(\"pimadata.csv\", delimiter=\",\")\n",
    "\n",
    "# Split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:09.461808Z",
     "start_time": "2018-05-14T21:13:09.456083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define your neural network model as a function\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:09.466444Z",
     "start_time": "2018-05-14T21:13:09.463561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a KerasClassifier model using your model definition function as an argument\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:28.224716Z",
     "start_time": "2018-05-14T21:13:09.467801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate your model using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:28.231859Z",
     "start_time": "2018-05-14T21:13:28.226450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f'10-fold cross validation scores:')\n",
    "for result in results: print('\\t', result.round(3))\n",
    "print('', f'mean: {results.mean().round(3)}', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:03:18.305433Z",
     "start_time": "2018-05-14T19:03:16.471025Z"
    }
   },
   "source": [
    "## [5] Keras Neural Network Hyperparameter Searching with GridSearchCV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on code at: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes we want to perform a hyperparameter search for our neural network.  Again to the rescue, Scikit-Learn has a class for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:28.235818Z",
     "start_time": "2018-05-14T21:13:28.233346Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do a grid search for the best results for combinations of epochs and batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:28.244724Z",
     "start_time": "2018-05-14T21:13:28.237567Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Define a function to create model (required for KerasClassifier)\n",
    "def create_model_1():\n",
    "    # create\n",
    "    model = Sequential()\n",
    "    # define\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:28.249005Z",
     "start_time": "2018-05-14T21:13:28.246576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the model\n",
    "\n",
    "# We don't include epochs and batch size during the model creation process because that's what we're searching\n",
    "model_1 = KerasClassifier(build_fn=create_model_1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:13:28.253661Z",
     "start_time": "2018-05-14T21:13:28.250393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Define the grid search parameters\n",
    "param_grid_1 = dict(batch_size=[10, 40, 80], epochs=[10, 50])\n",
    "print(param_grid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:09.565380Z",
     "start_time": "2018-05-14T21:13:28.255318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Perform the grid search\n",
    "grid_1 = GridSearchCV(estimator=model_1, param_grid=param_grid_1, n_jobs=1)\n",
    "grid_result_1 = grid_1.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:09.576652Z",
     "start_time": "2018-05-14T21:14:09.567084Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Step 5: Summarize the results\n",
    "def summarize_results(grid_result):\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    print('Results')\n",
    "    for mean, param in zip(means, params):\n",
    "        print(f'{param} -- {mean.round(3)}')\n",
    "    print('')\n",
    "    print(f'Best: {grid_result.best_score_.round(3)} using {grid_result.best_params_}')\n",
    "summarize_results(grid_result_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, let's do the same thing again, but this time let's try the difference between different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:09.583264Z",
     "start_time": "2018-05-14T21:14:09.578050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define a function to create model (required for KerasClassifier)\n",
    "\n",
    "# This time the parameter we are searching for is in the model definition, \n",
    "# so we have to include it as an argument (\"optimizer\")\n",
    "def create_model_2(optimizer='Adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:09.587850Z",
     "start_time": "2018-05-14T21:14:09.584880Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 2: Create the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# We're not searching for epoch or batch size, so we have to include values for them\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m KerasClassifier(build_fn\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_model_2\u001b[49m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model_2' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: Create the model\n",
    "\n",
    "# We're not searching for epoch or batch size, so we have to include values for them\n",
    "model_2 = KerasClassifier(build_fn=create_model_2, epochs=2, batch_size=40, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:09.591954Z",
     "start_time": "2018-05-14T21:14:09.589568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Define the grid search parameters\n",
    "param_grid_2 = dict(optimizer=['SGD', 'Adam', 'RMSprop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:24.129511Z",
     "start_time": "2018-05-14T21:14:09.593702Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Perform the grid search\n",
    "grid_2 = GridSearchCV(estimator=model_2, param_grid=param_grid_2, n_jobs=1)\n",
    "grid_result_2 = grid_2.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:24.134159Z",
     "start_time": "2018-05-14T21:14:24.131153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "{'optimizer': 'SGD'} -- 0.635\n",
      "{'optimizer': 'Adam'} -- 0.579\n",
      "{'optimizer': 'RMSprop'} -- 0.576\n",
      "\n",
      "Best: 0.635 using {'optimizer': 'SGD'}\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Summarize the results\n",
    "summarize_results(grid_result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can do this for pretty much any set of parameters, like weight initializations or anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:24.141603Z",
     "start_time": "2018-05-14T21:14:24.135910Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def create_model_3(init_mode='uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:24.146554Z",
     "start_time": "2018-05-14T21:14:24.143960Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "model_3 = KerasClassifier(build_fn=create_model_3, epochs=2, batch_size=40, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:24.150951Z",
     "start_time": "2018-05-14T21:14:24.148346Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "param_grid_3 = dict(init_mode=['uniform', 'normal', 'zero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:42.131095Z",
     "start_time": "2018-05-14T21:14:24.152286Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4\n",
    "grid_3 = GridSearchCV(estimator=model_3, param_grid=param_grid_3, n_jobs=1)\n",
    "grid_result_3 = grid_3.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T21:14:42.135366Z",
     "start_time": "2018-05-14T21:14:42.132685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "{'init_mode': 'uniform'} -- 0.651\n",
      "{'init_mode': 'normal'} -- 0.652\n",
      "{'init_mode': 'zero'} -- 0.651\n",
      "\n",
      "Best: 0.652 using {'init_mode': 'normal'}\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "summarize_results(grid_result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
